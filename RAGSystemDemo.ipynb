{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alwin-Lin/studentTesting/blob/main/RAGSystemDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install & import"
      ],
      "metadata": {
        "id": "_UoCROz2L_Yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!pip install google-generativeai\n",
        "!pip install chromadb\n",
        "!pip install typing"
      ],
      "metadata": {
        "id": "haYhUCaCL9-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pypdf import PdfReader\n",
        "import os\n",
        "import re\n",
        "import google.generativeai as genai\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "import chromadb\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "HF6wiQIbMQ3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the PDF from the specified URL and save it to the given path\n"
      ],
      "metadata": {
        "id": "asxN9hugMVqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_pdf(url, save_path):\n",
        "    response = requests.get(url)\n",
        "    with open(save_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "# URL and local path for the PDF document\n",
        "pdf_url = \"https://services.google.com/fh/files/misc/ai_adoption_framework_whitepaper.pdf\"\n",
        "pdf_path = \"ai_adoption_framework_whitepaper.pdf\"\n",
        "download_pdf(pdf_url, pdf_path)\n",
        "\n",
        "# Load the PDF file and extract text from each page\n",
        "def load_pdf(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            text += page_text\n",
        "    return text\n",
        "\n",
        "pdf_text = load_pdf(pdf_path)"
      ],
      "metadata": {
        "id": "t5HPleazMeGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set the API key"
      ],
      "metadata": {
        "id": "fr9S9EzmMhke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add your API_KEY to the secrets manager in the left panel \"ðŸ”‘\"."
      ],
      "metadata": {
        "id": "j-FUvthxMjvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set and validate the API key for Gemini API\n",
        "os.environ['GEMINI_API_KEY'] = 'lamoxd'\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"Gemini API Key not provided or incorrect. Please provide a valid GEMINI_API_KEY.\")\n",
        "try:\n",
        "    genai.configure(api_key=gemini_api_key)\n",
        "    print(\"API configured successfully with the provided key.\")\n",
        "except Exception as e:\n",
        "    print(\"Failed to configure API:\", str(e))"
      ],
      "metadata": {
        "id": "8CCqb50rMkaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb180e8-59cb-4b5e-86ab-f4bc33ef1925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API configured successfully with the provided key.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preperation for embeding"
      ],
      "metadata": {
        "id": "CbHGWitYR3bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text into chunks based on double newlines\n",
        "def split_text(text):\n",
        "    return [i for i in re.split('\\n\\n', text) if i.strip()]\n",
        "\n",
        "chunked_text = split_text(pdf_text)\n",
        "\n",
        "# Define a custom embedding function using Gemini API\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "        genai.configure(api_key=gemini_api_key)\n",
        "        model = \"models/embedding-001\"\n",
        "        title = \"Custom query\"\n",
        "        return genai.embed_content(model=model, content=input, task_type=\"retrieval_document\", title=title)[\"embedding\"]"
      ],
      "metadata": {
        "id": "HdUj21BySDXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chroma DB"
      ],
      "metadata": {
        "id": "rAg7cXoLSHIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory for database if it doesn't exist\n",
        "db_folder = \"chroma_db\"\n",
        "if not os.path.exists(db_folder):\n",
        "    os.makedirs(db_folder)\n",
        "\n",
        "# Create a Chroma database with the given documents\n",
        "def create_chroma_db(documents: List[str], path: str, name: str):\n",
        "    chroma_client = chromadb.PersistentClient(path=path)\n",
        "    db = chroma_client.create_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
        "    for i, d in enumerate(documents):\n",
        "        db.add(documents=[d], ids=[str(i)])\n",
        "    return db, name\n",
        "\n",
        "# Specify the path and collection name for Chroma database\n",
        "db_name = \"rag_experiment\"\n",
        "db_path = os.path.join(os.getcwd(), db_folder)\n",
        "db, db_name = create_chroma_db(chunked_text, db_path, db_name)\n",
        "\n",
        "# Load an existing Chroma collection\n",
        "def load_chroma_collection(path: str, name: str):\n",
        "    chroma_client = chromadb.PersistentClient(path=path)\n",
        "    return chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
        "\n",
        "db = load_chroma_collection(db_path, db_name)\n",
        "\n",
        "# Retrieve the Top-N relevant passages based on the query\n",
        "def get_relevant_passage(query: str, db, n_results: int):\n",
        "    results = db.query(query_texts=[query], n_results=n_results)\n",
        "    return [doc[0] for doc in results['documents']]\n",
        "\n",
        "query = \"What is the AI Maturity Scale?\"\n",
        "relevant_text = get_relevant_passage(query, db, n_results=3)"
      ],
      "metadata": {
        "id": "PiubA_pwSGeQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "57e07b3f-fd9c-4db3-b842-dbdf4c669ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8eacf9d6f647>:9: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
            "  db = chroma_client.create_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "Collection [rag_experiment] already exists",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-8eacf9d6f647>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdb_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"rag_experiment\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdb_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_chroma_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunked_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Load an existing Chroma collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-8eacf9d6f647>\u001b[0m in \u001b[0;36mcreate_chroma_db\u001b[0;34m(documents, path, name)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_chroma_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mchroma_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPersistentClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchroma_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGeminiEmbeddingFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/client.py\u001b[0m in \u001b[0;36mcreate_collection\u001b[0;34m(self, name, configuration, metadata, embedding_function, data_loader, get_or_create)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mconfiguration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding_function\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         model = self._server.create_collection(\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/rust.py\u001b[0m in \u001b[0;36mcreate_collection\u001b[0;34m(self, name, configuration, metadata, get_or_create, tenant, database)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mconfiguration_json_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         collection = self.bindings.create_collection(\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfiguration_json_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_or_create\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtenant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         )\n",
            "\u001b[0;31mInternalError\u001b[0m: Collection [rag_experiment] already exists"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt construction"
      ],
      "metadata": {
        "id": "tIw_2y0MSPbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a prompt for the generation model based on the query and retrieved data\n",
        "def make_rag_prompt(query: str, relevant_passage: str):\n",
        "    escaped_passage = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
        "    prompt = f\"\"\"You are a friendly and supportive tutor that helps users find answers by guiding them with questions and hints.\n",
        "Focus on encouraging exploration and critical thinking, rather than providing direct answers.\n",
        "Use simple language and break down any complex concepts to ensure clarity.\n",
        "Aim to foster a learning environment where users feel comfortable engaging with the material and discovering the answers themselves.\n",
        "Do not answer qeustions that are not realted to the documents.\n",
        "Do not make up things that does not exsist within the documents.\n",
        "QUESTION: '{query}'\n",
        "PASSAGE: '{escaped_passage}'\n",
        "\n",
        "ANSWER:\n",
        "\"\"\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "pfJok75TSOsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Answers"
      ],
      "metadata": {
        "id": "IOzmkYOgSYV5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ej1wyerL4Ve",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "3784e065-3808-41f9-8d11-33102d51abd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The AI Maturity Scale is a tool that helps you evaluate the six AI maturity themes (Learn, Lead, Access, Scale, Secure, Automate) across three phases: Tactical, Strategic, and Transformational.\n",
            "\n",
            "Please enter your query: What is the The AI maturity phases?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-8eacf9d6f647>:22: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
            "  return chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Answer: The AI maturity phases are:\n",
            "\n",
            "*   Tactical\n",
            "*   Strategic\n",
            "*   Transformational\n",
            "\n",
            "You can find more details about each phase in the document under the section \"The AI maturity phases.\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate an answer using the Gemini Pro API\n",
        "def generate_answer(prompt: str):\n",
        "    gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "    if not gemini_api_key:\n",
        "        raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
        "    genai.configure(api_key=gemini_api_key)\n",
        "    model = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
        "    result = model.generate_content(prompt)\n",
        "    return result.text\n",
        "\n",
        "# Construct the prompt and generate the answer\n",
        "final_prompt = make_rag_prompt(query, \"\".join(relevant_text))\n",
        "answer = generate_answer(final_prompt)\n",
        "print(answer)\n",
        "\n",
        "# Interactive function to process user input and generate an answer\n",
        "def process_query_and_generate_answer():\n",
        "    query = input(\"Please enter your query: \")\n",
        "    if not query:\n",
        "        print(\"No query provided.\")\n",
        "        return\n",
        "    db = load_chroma_collection(db_path, db_name)\n",
        "    relevant_text = get_relevant_passage(query, db, n_results=1)\n",
        "    if not relevant_text:\n",
        "        print(\"No relevant information found for the given query.\")\n",
        "        return\n",
        "    final_prompt = make_rag_prompt(query, \"\".join(relevant_text))\n",
        "    answer = generate_answer(final_prompt)\n",
        "    print(\"Generated Answer:\", answer)\n",
        "\n",
        "# Invoke the function to interact with user\n",
        "process_query_and_generate_answer()"
      ]
    }
  ]
}